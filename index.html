<!doctype html>
<html lang="en">
<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap CSS -->
	<link href="resources/bootstrap.min.css" rel="stylesheet">
	<title>Unsupervised Volumetric Animation</title>
</head>
<body>

	<section class="jumbotron text-center">
		<div class="container">
			<h1 class="jumbotron-heading">Unsupervised Volumetric Animation</h1>
			<h2 class="jumbotron-heading">Supplementary Material</h1>

	</div>
	</section>
        <div class="container pt-5">
                        <p class="lead"> 
			In this supplement we provide additional qualitative results and comparisons. If videos do not play on your machine, please unzip the folder locally, go to the video_sequences folder and play videos manually.</p>
	
			<h2>Animation results on VoxCeleb</h2>
			<p class="lead"> Here we show animation results rendered under novel views within the [-15°,15°] range. Our method uses a single image and a driving sequence to synthesize animations. Additionally, we show depth, normals and parts (LBS weights) predicted by our method in an unsupervised way. Note, that our method successfully renders wide range pose changes and diverse object shapes. </p>

                        <div class="row">
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/animations/vox-example-1.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/animations/vox-example-2.mp4" type="video/mp4" />
					</video>
				</div>
			</div>
			<br/>
			<h2>Animation results on TEDXPeople</h2>
			<p class="lead"> Since our method is unsupervised, it can be successfully trained on bodies too, enabling unsupervised animation and novel view synthesis. Note that our method predicts rich geometry and identifies meaningful object parts, which correspond to hands, face, left and right shoulders, torso.</p>
                        <div class="row">
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/animations/tedx-example-3.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/animations/tedx-example-4.mp4" type="video/mp4" />
					</video>
				</div>
			</div>

			<br/>
			<h2>Comparison with MRAA and LIA on VoxCeleb</h2>
			<p class="lead">
			Here we compare our method with two state-of-the-art animation 2D methods LIA and MRAA. We report typical examples of animations generated by each of the methods. Since our method uses a 3D representation for animation it better preserves face proportions and head poses. MRAA significantly alters the shape and does not faithfully convey expressions. LIA slightly changes the proportions, while smoothing the image and changing its color. 
			</p>
                        <div class="row">
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/comparisons/vox-example-1.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/comparisons/vox-example-2.mp4" type="video/mp4" />
					</video>
				</div>
			</div>


			<br/>
			<h2>Comparison with MRAA on TEDXPeople</h2>
			<p class="lead"> 
			Here we compare with MRAA trained on the TEDXPeople dataset. The example on the right is particularly challenging as in its driving video a person is playing guitar, which is rarely observed in the dataset. Despite this our method can robustly detect the poses of parts, faithfully animating source images. MRAA detects guitar as a part of the body, substantially altering the proportions of the body in the animated sequence. 
			</p>

                        <div class="row">
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/comparisons/tedx-example-2.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/comparisons/tedx-example-1.mp4" type="video/mp4" />
					</video>
				</div>
			</div>

  			<br/>

			<h2>Novel-view synthesis on VoxCeleb</h2>
			<p class="lead"> Here we show a reconstruction made from a single image and rotate it along the y-axis, showing a wide range of novel views. We also show the corresponding depth, normals and LBS weights. </p>

                        <div class="row">
                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/novel-view-synthesis/vox-example.mp4" type="video/mp4" />
					</video>
				</div>
			</div>


			<br/>
			<h2>Novel-view synthesis on TEDXPeople</h2>
			<p class="lead"> Similarly for bodies, our method produces novel views, depth, normals and parts in an unsupervised manner. We report a slightly narrower range of poses compared to faces, as the distribution of body poses in TEDXPeople is biased towards frontal poses. </p>

                        <div class="row">
                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/novel-view-synthesis/tedx-example.mp4" type="video/mp4" />
					</video>
				</div>
			</div>

 			<br/>
			<h2>Novel-view synthesis on cats</h2>
			<p class="lead"> Here the model is trained on <strong>images only</strong>. Despite this, due to the 3D inductive bias provided by PnP, our method discovers meaningful geometry even in this challenging case. </p>

                        <div class="row">
                                <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/cats/example-1.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/cats/example-2.mp4" type="video/mp4" />
					</video>
				</div>
	
			</div>


  			<br/>

			<h2>Comparison of direct pose prediction and PnP-based.</h2>
			<p class="lead"> We argue that the proposed framework involving differentiable PnP favors discovery of correct 3D geometry. To show it we give qualitative samples of the model using PnP and the one predicting the pose of each part directly, using a neural network. In this experiment we use the result of <i>G-phase</i>, where only a single part is learned. The <i>Direct</i> method learned flat geometry, while our PnP-based method produced plausible geometry with small details, including hair and wrinkles.</p>

                        <div class="row">
                                <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/pnp-vs-direct/example-1.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/pnp-vs-direct/example-2.mp4" type="video/mp4" />
					</video>
				</div>
	                        <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/pnp-vs-direct/example-3.mp4" type="video/mp4" />
					</video>
				</div>
	                        <div class="col-md-6" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/pnp-vs-direct/example-4.mp4" type="video/mp4" />
					</video>
				</div>
	
			</div>


  			<br/>



			<h2>Random generation on VoxCeleb</h2>
			<p class="lead"> Our framework learns a canonical latent space, allowing us to synthesize new unseen identities. Here we sample random identity from the latent space (i.e. use standard normal noise as embedding), and animate it using the driving from the test set. Note, that our framework is auto-decoder-based, hence, the quality of the texture is inferior to GAN-based methods. Recall that our framework is non-adversarial and is trained using reconstruction losses only. Despite this artifact the method is able to generate reasonable geometry even for random samples.</p>

                        <div class="row">
                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/vox-example-1.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/vox-example-2.mp4" type="video/mp4" />
					</video>
				</div>
                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/vox-example-3.mp4" type="video/mp4" />
					</video>
				</div>

                                <div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/vox-example-4.mp4" type="video/mp4" />
					</video>
				</div>

			</div>

   			<br/>
			<h2>Random generation on TEDXPeople</h2>
			<div class="row">
				<div class="col-md-8" style="display: block; margin-left: auto; margin-right: auto;">
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/tedx-example-1.mp4" type="video/mp4" />
					</video>
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/tedx-example-2.mp4" type="video/mp4" />
					</video>
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/tedx-example-3.mp4" type="video/mp4" />
					</video>
					<video class="video-fluid w-100" controls autoplay loop muted>
						<source src="video_sequences/random/tedx-example-4.mp4" type="video/mp4" />
					</video>
				</div>

			</div>

    
                 
	</div>
</body>
</html>
